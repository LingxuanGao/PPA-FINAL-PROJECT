[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Final Project Portfolio",
    "section": "",
    "text": "This portfolio documents our final project of Eviction Risk Prediction in Philadelphia in Public Policy Analytics (MUSA 5080).\n\n\nName: Lingxuan Gao, Xiaoqing Chen\n\n\n\n\nEmail: [gao459@upenn.edu]"
  },
  {
    "objectID": "index.html#about-our-team",
    "href": "index.html#about-our-team",
    "title": "MUSA 5080 Final Project Portfolio",
    "section": "",
    "text": "Name: Lingxuan Gao, Xiaoqing Chen"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Final Project Portfolio",
    "section": "",
    "text": "Email: [gao459@upenn.edu]"
  },
  {
    "objectID": "Eviction.html",
    "href": "Eviction.html",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "",
    "text": "As Philadelphia approaches 2026, the city faces an unprecedented “housing cliff” driven by federal policy shocks. New mandates are shifting away from the “Housing First” model and enforcing stricter NSPIRE inspection standards, threatening the funding for approximately 1,200 permanent supportive housing units and risking mass disqualifications of affordable stock.\nIn this volatile landscape, relying solely on historical averages is insufficient. This project develops a spatial predictive model to forecast eviction risk for the Nov 2025 – Nov 2026 period. By integrating structural distress indicators (L&I violations), spatial context, and demographic data, we aim to identify the specific census tracts most vulnerable to this new wave of instability, enabling a “High Precision” allocation of limited legal aid resources."
  },
  {
    "objectID": "Eviction.html#load-libraries",
    "href": "Eviction.html#load-libraries",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(scales)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)"
  },
  {
    "objectID": "Eviction.html#define-themes",
    "href": "Eviction.html#define-themes",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "Eviction.html#load-and-clean-all-the-datasets",
    "href": "Eviction.html#load-and-clean-all-the-datasets",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "Load and clean all the datasets",
    "text": "Load and clean all the datasets\n\n\nCode\n# Monthly totals vs baseline (city-wide)\nphilly_barchart &lt;- read_csv(\"data/philadelphia_barchart.csv\") %&gt;%\n  mutate(\n    date = my(month)\n  )\n\n# Claims severity over time\nphilly_claims_monthly &lt;- read_csv(\"data/philadelphia_claims_monthly.csv\") %&gt;%\n  mutate(\n    month_date = ymd(month_date)\n  )\n\n# Group-level (race, gender, etc.) monthly trends\nphilly_linechart &lt;- read_csv(\"data/philadelphia_linechart.csv\") %&gt;%\n  mutate(\n    month_date = my(month)\n  )\n\n# Census tract map snapshot\nphilly_map &lt;- read_csv(\"data/philadelphia_map.csv\") %&gt;%\n  mutate(\n   GEOID = as.character(id),\n    month_date = dmy(month_date)\n  )\n\n# Tract-level monthly pre/post pandemic\nphilly_monthly &lt;- read_csv(\"data/philadelphia_monthly_2020_2021.csv\") %&gt;%\n  mutate(\n    month_date = my(month)\n  )\n\n# Tract-level weekly \nweekly &lt;- read_csv(\"data/philadelphia_weekly_2020_2021.csv\") %&gt;%\n  mutate(\n    week_date = ymd(week_date)\n  )\n\n# Code / license violations (point data, older)\nli_violations &lt;- read_csv(\"data/li_violations.csv\")\n\n# Hotspot properties (top filers)\nhotspots &lt;- read_csv(\"data/philadelphia_hotspots_media_report.csv\") %&gt;%\n  mutate(\n    time_period = ymd(time_period),\n    end_date    = ymd(end_date)\n  )\n\n\nData sources:\nPrimary dataset: Eviction Lab – Philadelphia Tracking\n\nCity-level series\nTract-level monthly data over multiple years\nTract-level snapshot of Philadelphia eviction risk (since Nov 2024)\nGroup-level disparities\nlandlord hotspots"
  },
  {
    "objectID": "Eviction.html#citywide-filings-over-time",
    "href": "Eviction.html#citywide-filings-over-time",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "2.1 Citywide filings over time",
    "text": "2.1 Citywide filings over time\nWe reorganized the monthly eviction-filing counts by grouping each month into a season (Winter, Spring, Summer, Fall). We computed four separate seasonal baselines:\nthe historical average for Winter/Spring/Summer/Fall months\nThen we produced a four-panel faceted plot to let us compare each season to what is “normal” for that season.\n\n\nCode\n# Add season column\nphilly_barchart2 &lt;- philly_barchart %&gt;%\n  mutate(\n    # extract month number\n    m = month(date),\n    season = case_when(\n      m %in% c(12, 1, 2) ~ \"Winter\",\n      m %in% c(3, 4, 5)  ~ \"Spring\",\n      m %in% c(6, 7, 8)  ~ \"Summer\",\n      m %in% c(9, 10, 11) ~ \"Fall\"\n    )\n  )\n\n# Calculate true seasonal averages\nseason_baseline &lt;- philly_barchart2 %&gt;%\n  group_by(season) %&gt;%\n  summarise(season_avg = mean(month_filings, na.rm = TRUE))\n\nseason_baseline\n\n\n# A tibble: 4 × 2\n  season season_avg\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Fall        1008.\n2 Spring       795.\n3 Summer       873.\n4 Winter      1115.\n\n\n\n\nCode\nggplot(philly_barchart2, aes(x = date, y = month_filings)) +\n  geom_col(fill = \"#3182bd\") +\n  geom_hline(\n    data = season_baseline,\n    aes(yintercept = season_avg),\n    linetype = \"dashed\"\n  ) +\n  facet_wrap(~ season, ncol = 2, scales = \"free_y\") +\n  labs(\n    title = \"Monthly Eviction Filings by Season\",\n    subtitle = \"Each panel shows filings and seasonal baseline\",\n    x = \"Date\",\n    y = \"Eviction filings\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 13),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nPatterns:\n1.Eviction filings were heavily suppressed across all seasons in 2020–2021. Every season shows extremely low activity during the early pandemic period.\n2.Beginning in 2022, filings rebound sharply—but not uniformly across seasons. -Summer 2022 displays one of the largest spikes in the dataset. -Fall and Spring quickly rise back to their historical levels. -Winter rebounds more unevenly but eventually stabilizes. So the seasonal cut reveals that the return to “normal” was staggered, with some seasons experiencing larger surges than others.\n3.From 2023 onward, virtually every season settles at or slightly above its long-term seasonal baseline. This indicates a new post-pandemic equilibrium where eviction activity has stabilized."
  },
  {
    "objectID": "Eviction.html#tract-level-risk-and-spatial-patterns",
    "href": "Eviction.html#tract-level-risk-and-spatial-patterns",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "2.2 Tract-level risk and spatial patterns",
    "text": "2.2 Tract-level risk and spatial patterns\nWe uses the multi-year tract-level monthly data to see how skewed filings are across tracts.\n\n\nCode\ntract_annual &lt;- philly_monthly %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(\n    total_filings = sum(filings_counts, na.rm = TRUE),\n    mean_monthly  = mean(filings_counts, na.rm = TRUE),\n    n_months      = n(),\n    .groups = \"drop\"\n  )\n\n# Add a vertical line at the 80th percentile (top 20%)\np80 &lt;- quantile(tract_annual$mean_monthly, 0.80, na.rm = TRUE)\n\nggplot(tract_annual, aes(x = mean_monthly)) +\n  geom_histogram(bins = 30, fill = \"#3182bd\") +\n  geom_vline(xintercept = p80, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Distribution of Mean Monthly Eviction Filings per Tract\",\n    subtitle = paste0(\"Dashed line = 80th percentile (candidate high-risk cutoff: \",\n                      round(p80, 2), \" filings/month)\"),\n    x = \"Mean monthly filings (multi-year)\",\n    y = \"Number of tracts\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nThis histogram shows that most census tracts have very low average eviction activity—often fewer than 2 filings per month.A small number of tracts sit far to the right, with 5–10+ filings per month, meaning they experience consistently high eviction pressure over multiple years. So eviction risk in Philly is highly concentrated, not evenly spread, which supports focusing limited legal aid and rent subsidies on that right-tail group of chronic hotspot tracts.\n\nLoad Spatial Data\n\n\nCode\n# Read your GeoJSONs\nboundary &lt;- st_read(\"data/City_Limits.geojson\")\n\n\nReading layer `City_Limits' from data source \n  `D:\\MUSA\\MUSA_5080\\PPA-FINAL-PROJECT\\data\\City_Limits.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.28031 ymin: 39.86747 xmax: -74.95575 ymax: 40.13793\nGeodetic CRS:  WGS 84\n\n\nCode\npa_tracts &lt;- st_read(\"data/PA-tracts.geojson\")\n\n\nReading layer `PA-tracts' from data source \n  `D:\\MUSA\\MUSA_5080\\PPA-FINAL-PROJECT\\data\\PA-tracts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 3217 features and 398 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.51989 ymin: 39.7198 xmax: -74.68952 ymax: 42.26986\nGeodetic CRS:  WGS 84\n\n\nCode\n# Join eviction data with tract geometry\nphilly_tracts &lt;- pa_tracts %&gt;%\n  filter(str_starts(GEOID, \"42101\"))\n\nphilly_map &lt;- philly_map %&gt;%\n  mutate(id = as.character(id))\n\nphilly_tracts &lt;- philly_tracts %&gt;%\n  mutate(GEOID = as.character(GEOID))\n\n\nmap_sf &lt;- philly_tracts %&gt;%\n  left_join(philly_map, by = c(\"GEOID\" = \"id\"))\n\n\n\n\nEviction rate map since Nov 2024\n\n\nCode\nggplot(\n  map_sf %&gt;% filter(!is.na(month_rate))\n) +\n  geom_sf(aes(fill = month_rate), color = NA) +\n  scale_fill_viridis(option = \"magma\", direction = -1) +\n  labs(\n    title = \"Eviction Filing Rate by Census Tract (Nov 2024–Nov 2025)\",\n    fill  = \"Eviction Rate\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title      = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nmap_compare &lt;- map_sf %&gt;%\n  mutate(\n    # month_diff is a ratio vs baseline\n    diff_vs_baseline = month_diff - 1\n  )\n\nggplot(\n  map_compare %&gt;% filter(!is.na(diff_vs_baseline))\n) +\n  geom_sf(aes(fill = diff_vs_baseline), color = NA) +\n  scale_fill_gradient2(\n    low  = \"#4575b4\",\n    mid  = \"white\",\n    high = \"#d73027\",\n    midpoint = 0,\n    labels = percent_format(accuracy = 1),\n    name = \"Change vs baseline\\n(2023–24)\"\n  ) +\n  labs(\n    title    = \"Eviction Filing Rate by Census Tract\",\n    subtitle = \"Nov 2024–Nov 2025 vs typical 2023–2024 rate\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nPhilly isn’t on fire everywhere—a handful of neighborhoods are overheating while others are holding steady or cooling down. Those red clusters are exactly where we’d point legal aid and rent relief first.\n\n\nPersistence of high-risk tracts\n\n\nCode\nthreshold &lt;- quantile(map_sf$month_rate, 0.8, na.rm = TRUE)\n\ntract_risk &lt;- map_sf %&gt;%\n  st_drop_geometry() %&gt;%                        # strip geometry\n  mutate(high = if_else(month_rate &gt;= threshold, 1L, 0L)) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(\n    times_high = sum(high, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nrisk_sf &lt;- philly_tracts %&gt;%\n  left_join(tract_risk, by = \"GEOID\")          # now y is NOT sf → OK\n\nggplot(risk_sf) +\n  geom_sf(aes(fill = times_high), color = NA) +\n  scale_fill_viridis_c(option = \"inferno\") +\n  labs(\n    title = \"Persistence of High Eviction Risk Across Tracts\",\n    fill  = \"Months above 80th percentile\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nHotspot landlords (top eviction filers)\n\n\nCode\nhotspot_sf &lt;- st_as_sf(\n  hotspots,\n  coords = c(\"lon\", \"lat\"),\n  crs = 4326\n)\n\nggplot() +\n  geom_sf(data = boundary, fill = \"grey95\", color = NA) +\n  geom_sf(data = hotspot_sf, aes(size = filings), alpha = 0.6, color = \"#d7301f\") +\n  scale_size(range = c(1, 10)) +\n  labs(\n    title = \"Top Eviction-Filing Landlords in Philadelphia\",\n    subtitle = \"Bubble size shows number of filings\",\n    size = \"Filings\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe map reveals that eviction activity is not driven solely by tenant poverty, but also by landlord behavior. The large red bubbles represent the specific locations of the city’s top filers. We observe that high-volume eviction filing is not evenly distributed; it is highly concentrated among a specific subset of property owners. The spatial pattern is stark and non-random. The largest clusters of high-volume filers are located in: North Philadelphia and West Philadelphia."
  },
  {
    "objectID": "Eviction.html#base-tract-list",
    "href": "Eviction.html#base-tract-list",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "3.1 Base tract list",
    "text": "3.1 Base tract list\n\n\nCode\n# One row per tract ID from the joined spatial layer\ntract_base &lt;- map_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(GEOID) %&gt;%\n  arrange(GEOID)"
  },
  {
    "objectID": "Eviction.html#add-acs-demographics-housing",
    "href": "Eviction.html#add-acs-demographics-housing",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "3.2 Add ACS demographics & housing",
    "text": "3.2 Add ACS demographics & housing\n\n\nCode\nacs_vars &lt;- c(\n  total_pop  = \"B01003_001\",  # total population\n  pov_total  = \"B17001_001\",  # poverty universe\n  pov_below  = \"B17001_002\",  # below poverty\n  occ_total  = \"B25003_001\",  # occupied housing units\n  occ_renter = \"B25003_003\",  # renter-occupied units\n  med_rent   = \"B25064_001\",  # median gross rent\n  white      = \"B03002_003\",\n  black      = \"B03002_004\",\n  hispanic   = \"B03002_012\"\n)\n\nacs_philly &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"PA\",\n  county    = \"Philadelphia\",\n  survey    = \"acs5\",\n  year      = 2022,\n  variables = acs_vars,\n  output    = \"wide\",\n  geometry  = FALSE\n) %&gt;%\n  transmute(\n    GEOID,\n    total_pop    = total_popE,\n    pov_rate     = if_else(pov_totalE &gt; 0, pov_belowE / pov_totalE, NA_real_),\n    renter_share = if_else(occ_totalE &gt; 0, occ_renterE / occ_totalE, NA_real_),\n    med_rent     = med_rentE,\n    pct_black    = if_else(total_popE &gt; 0, blackE / total_popE, NA_real_),\n    pct_hispanic = if_else(total_popE &gt; 0, hispanicE / total_popE, NA_real_)\n  )\n\n# Attach ACS to tract base and to the spatial object\ntract_context &lt;- tract_base %&gt;%\n  left_join(acs_philly, by = \"GEOID\")\n\nmap_sf_context &lt;- map_sf %&gt;%       # or map_sf_context if you’d already created it\n  left_join(acs_philly, by = \"GEOID\")"
  },
  {
    "objectID": "Eviction.html#add-landlord-hotspot-intensity",
    "href": "Eviction.html#add-landlord-hotspot-intensity",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "3.3 Add landlord hotspot intensity",
    "text": "3.3 Add landlord hotspot intensity\n\n\nCode\n# Hotspot landlords as points (you already use this for the bubble map)\nhotspot_sf &lt;- st_as_sf(\n  hotspots,\n  coords = c(\"lon\", \"lat\"),\n  crs = 4326\n) %&gt;%\n  st_transform(st_crs(map_sf_context))\n\n# Spatial join: assign each hotspot property to a tract\nhotspots_by_tract &lt;- st_join(\n  hotspot_sf,\n  map_sf_context[\"GEOID\"],\n  left = FALSE\n) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(\n    hotspot_filings     = sum(filings, na.rm = TRUE),\n    n_hotspot_props     = n(),\n    n_hotspot_landlords = n_distinct(xplaintiff),\n    .groups = \"drop\"\n  )\n\nmap_sf_context &lt;- map_sf_context %&gt;%\n  left_join(hotspots_by_tract, by = \"GEOID\") %&gt;%\n  mutate(\n    across(\n      c(hotspot_filings, n_hotspot_props, n_hotspot_landlords),\n      ~ replace_na(., 0)\n    )\n  )"
  },
  {
    "objectID": "Eviction.html#add-historic-code-violation-intensity-2012-2016",
    "href": "Eviction.html#add-historic-code-violation-intensity-2012-2016",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "3.4 Add historic code-violation intensity (2012-2016)",
    "text": "3.4 Add historic code-violation intensity (2012-2016)\nTo capture long-term structural neglect, we transformed historical code violation data (2012–2016) from individual geographic points into a tract-level density metric. We spatially joined each violation instance to its corresponding census tract, aggregated the total counts, and normalized them by population to derive the ‘Violations per 1,000 Residents’ (viol_per_1k) variable. This serves as a standardized proxy for ‘legacy distress’ within the built environment, distinguishing chronic disrepair from recent instability.\n\n\nCode\n# Violations as points\nli_violations_sf &lt;- st_as_sf(\n  li_violations,\n  coords = c(\"lng\", \"lat\"),\n  crs = 4326\n) %&gt;%\n  st_transform(st_crs(map_sf_context))\n\n# Join each violation to a tract\nviol_by_tract &lt;- st_join(\n  li_violations_sf,\n  map_sf_context[\"GEOID\"],\n  left = FALSE\n) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(\n    violations_total = n(),\n    violations_types = n_distinct(violationdescription),\n    .groups = \"drop\"\n  )\n\nmap_sf_context &lt;- map_sf_context %&gt;%\n  left_join(viol_by_tract, by = \"GEOID\") %&gt;%\n  mutate(\n    violations_total = replace_na(violations_total, 0),\n    violations_types = replace_na(violations_types, 0),\n    viol_per_1k = if_else(\n      total_pop &gt; 0,\n      1000 * violations_total / total_pop,\n      NA_real_\n    )\n  )"
  },
  {
    "objectID": "Eviction.html#housing-related-complaint-data",
    "href": "Eviction.html#housing-related-complaint-data",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "3.5 311 housing-related complaint data",
    "text": "3.5 311 housing-related complaint data\nTo quantify localized housing distress, we processed raw 311 service request data by filtering for ten specific housing-related categories recorded during 2025. These individual incidents were spatially joined to census tracts to aggregate point-level data into neighborhood-level counts. Finally, we normalized these figures by total population to create a standardized ‘Complaints per 1,000 Residents’ metric (housing311_per_1k), serving as a dynamic proxy for property mismanagement and structural neglect.\n\n\nCode\nhousing_types &lt;- c(\n  \"Dangerous Building Complaint\",\n  \"Construction Complaints\",\n  \"Maintenance Complaint\",\n  \"Sanitation Violation\",\n  \"Sanitation / Dumpster Violation\",\n  \"Dumpster Violation\",\n  \"Fire Safety Complaint\",\n  \"Smoke Detector\",\n  \"Graffiti Removal\",\n  \"Homeless Encampment Request\"\n)\n\nhousing311_raw &lt;- read_csv(\"data/public_cases_fc.csv\") %&gt;%\n  clean_names() %&gt;%\n  filter(service_name %in% housing_types) %&gt;%\n  filter(!is.na(lon), !is.na(lat)) %&gt;%   # adjust if your columns are lon/lat\n  mutate(\n    request_date = as.Date(requested_datetime),   # change name if needed\n    year         = year(request_date)\n  ) %&gt;%\n  filter(year &gt;= 2020)               # recent years only\n\n\n\n\nCode\nhousing311_sf &lt;- housing311_raw %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(map_sf))\n\nhousing311_tract &lt;- st_join(\n  housing311_sf,\n  map_sf[\"GEOID\"],\n  left = FALSE\n)\n\nhousing311_by_tract &lt;- housing311_tract %&gt;%\n  st_drop_geometry() %&gt;%\n  count(GEOID, name = \"housing311_count\") %&gt;%\n  left_join(\n    tract_context %&gt;% select(GEOID, total_pop),\n    by = \"GEOID\"\n  ) %&gt;%\n  mutate(\n    housing311_per_1k = if_else(\n      total_pop &gt; 0,\n      1000 * housing311_count / total_pop,\n      NA_real_\n    )\n  ) %&gt;%\n  select(GEOID, housing311_per_1k)"
  },
  {
    "objectID": "Eviction.html#master-data-setup-feature-engineering",
    "href": "Eviction.html#master-data-setup-feature-engineering",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "4.1 Master Data Setup & Feature Engineering",
    "text": "4.1 Master Data Setup & Feature Engineering\n\n4.1.1 Define Custom Time Periods (The “Nov-Nov” Fix)\nTo align our historical training data with the target prediction window (November 2025 – November 2026), we engineered a custom time period variable (period_year). Standard calendar years (Jan–Dec) were replaced with ‘Eviction Years’ that run from November to October. For instance, data from November and December of 2020 were reassigned to the ‘2021 Period.’ This ensures that every year in our panel represents a full 12-month cycle that perfectly mirrors the seasonal structure of our final forecast.\n\n\nCode\n#  Prepare the Monthly Data with Custom Time Periods\npanel_setup &lt;- philly_monthly %&gt;% \n  mutate(\n    # Convert text to Date\n    month_date = my(month),\n    \n    # --- CRITICAL FIX: Custom \"Eviction Year\" Logic ---\n    # We want the year to start in November.\n    # If month is Nov (11) or Dec (12), we assign it to the NEXT year.\n    # Example: Nov 2020 -&gt; Period 2021\n    # Example: Nov 2024 -&gt; Period 2025 (The most recent complete year)\n    period_year = if_else(\n      month(month_date) &gt;= 11, \n      year(month_date) + 1, \n      year(month_date)\n    )\n  ) %&gt;%\n  # Filter out data before Nov 2020 to ensure we have full 12-month cycles\n  filter(period_year &gt;= 2021)\n\n\n\n\n4.1.2 Build Basic Tract-Year Panel\n\n\nCode\n# Aggregate from Monthly to Annual (Period) Level\nmodel_df &lt;- panel_setup %&gt;% \n  group_by(GEOID, period_year) %&gt;% \n  summarise(\n    # --- FIX: Variable Name Changed from filings_2020 ---\n    filings_total = sum(filings_counts, na.rm = TRUE),\n    mean_monthly  = mean(filings_counts, na.rm = TRUE),\n    \n    # Count how many months of data we have in this custom period\n    n_months      = n(),\n    .groups = \"drop\"\n  ) %&gt;% \n  \n  # Data Quality Check: Only keep periods with at least 10 months of data\n  filter(n_months &gt;= 10)\n\n\nHere we define what “High Risk” means and create the lag variable.\n\n\nCode\n# Create Target Variable (Y) and Lagged Predictor (X)\nmodel_df_lagged &lt;- model_df %&gt;% \n  # A. Define \"High Risk\" Threshold per Year\n  group_by(period_year) %&gt;% \n  mutate(\n    # Calculate the 80th percentile threshold for THIS specific period\n    cutoff80  = quantile(mean_monthly, 0.80, na.rm = TRUE),\n    \n    # Create Binary Target: 1 if this tract is in the top 20%, 0 otherwise\n    high_risk = if_else(mean_monthly &gt;= cutoff80, 1L, 0L)\n  ) %&gt;% \n  ungroup() %&gt;% \n  \n  # Create Lagged History (The most important predictor)\n  arrange(GEOID, period_year) %&gt;% \n  group_by(GEOID) %&gt;% \n  mutate(\n    # Previous year's mean monthly filings\n    lag_mean_monthly = lag(mean_monthly)\n  ) %&gt;% \n  ungroup() %&gt;% \n  \n  # Remove rows with no history (we can't train on the first year)\n  filter(!is.na(lag_mean_monthly))\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use 0.8 as the threshold because of The Pareto Principle / 80-20 Rule.\n\n\n\n\n4.1.3 Prepare External Features (Context)\n\n\nCode\n# A. Legacy Distress (Historic Violations 2012-2016)\nlegacy_distress_join &lt;- tract_context %&gt;% \n  select(GEOID, total_pop) %&gt;% \n  left_join(viol_by_tract, by = \"GEOID\") %&gt;% \n  mutate(\n    hist_viol_per_1k = if_else(total_pop &gt; 0, (violations_total / total_pop) * 1000, 0)\n  ) %&gt;%\n  select(GEOID, hist_viol_per_1k)\n\n# B. Hotspot Landlord Activity\nhotspot_join &lt;- hotspots_by_tract %&gt;% \n  select(GEOID, hotspot_filings)\n\n# C. 311 Housing Complaints\nhousing311_join &lt;- housing311_by_tract %&gt;% \n  select(GEOID, housing311_per_1k)\n\n# D. Spatial Context 1: Distance to City Hall\ntract_centroids &lt;- philly_tracts %&gt;% st_centroid()\ncity_hall &lt;- st_sfc(st_point(c(-75.1635, 39.9526)), crs = 4326) %&gt;% \n  st_transform(st_crs(tract_centroids))\n\ndist_center &lt;- st_distance(tract_centroids, city_hall)\n\ndist_center_join &lt;- data.frame(\n  GEOID = tract_centroids$GEOID,\n  dist_center_km = as.numeric(dist_center) / 1000\n)\n\n# E. Spatial Context 2: Distance to Nearest Hotspot\ndist_hotspot &lt;- st_distance(tract_centroids, hotspot_sf) \nmin_dist_hotspot &lt;- apply(dist_hotspot, 1, min)\n\ndist_hotspot_join &lt;- data.frame(\n  GEOID = tract_centroids$GEOID,\n  dist_hotspot_km = as.numeric(min_dist_hotspot) / 1000\n)\n\n# F. Neighborhood Typology (Clustering)\n# We cluster tracts based on static demographics to capture \"neighborhood type\"\ncluster_input &lt;- tract_context %&gt;%\n  st_drop_geometry() %&gt;%\n  select(GEOID, pov_rate, renter_share, med_rent, pct_black, pct_hispanic) %&gt;%\n  drop_na()\n\nset.seed(42)\nclusters &lt;- kmeans(scale(cluster_input %&gt;% select(-GEOID)), centers = 5)\ncluster_input$neighborhood_cluster &lt;- factor(clusters$cluster)\n\ncluster_join &lt;- cluster_input %&gt;% select(GEOID, neighborhood_cluster)\n\n\n\n\n4.1.4 Build Final Master Dataset\n\n\nCode\ntraining_data &lt;- model_df_lagged %&gt;% \n  # Join Demographics (ACS)\n  left_join(tract_context %&gt;% select(GEOID, pov_rate, renter_share, med_rent, pct_black, pct_hispanic), by = \"GEOID\") %&gt;%\n  \n  # Join External Predictors\n  left_join(legacy_distress_join, by = \"GEOID\") %&gt;%\n  left_join(hotspot_join, by = \"GEOID\") %&gt;%\n  left_join(housing311_join, by = \"GEOID\") %&gt;%\n  left_join(dist_center_join, by = \"GEOID\") %&gt;%\n  left_join(dist_hotspot_join, by = \"GEOID\") %&gt;%  \n  left_join(cluster_join, by = \"GEOID\") %&gt;%       \n  \n  # Handle NAs created by joins (fill counts with 0)\n  mutate(\n    hotspot_filings   = replace_na(hotspot_filings, 0),\n    hist_viol_per_1k  = replace_na(hist_viol_per_1k, 0),\n    housing311_per_1k = replace_na(housing311_per_1k, 0)\n  ) %&gt;%\n  \n  # Final cleanup\n  drop_na(pov_rate, renter_share)"
  },
  {
    "objectID": "Eviction.html#model-building",
    "href": "Eviction.html#model-building",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "4.2 Model Building",
    "text": "4.2 Model Building\n\n4.2.1 Train/Test Split\nWe employed a Time-Based Train/Test Split (Temporal Splitting) combined with Complete Case Analysis. Instead of randomly shuffling the data (which is standard for non-time-series problems), we split the dataset chronologically.\n\n\nCode\n# 1. Define Variables\n# Ensure we drop NAs for ALL potential variables to keep samples consistent across models\nall_vars &lt;- c(\"high_risk\", \"lag_mean_monthly\", \n              \"pov_rate\", \"renter_share\", \"med_rent\", \"pct_black\", \"pct_hispanic\",\n              \"hist_viol_per_1k\", \"hotspot_filings\", \"housing311_per_1k\",\n              \"dist_center_km\", \"dist_hotspot_km\", \"neighborhood_cluster\")\n\nmodel_data &lt;- training_data %&gt;%\n  drop_na(all_of(all_vars))\n\n# 2. Time-Based Split\ntrain_set &lt;- model_data %&gt;% filter(period_year &lt; 2025)\ntest_set  &lt;- model_data %&gt;% filter(period_year == 2025)\n\n\n\n\n4.2.2 Build 5 Competing Models\nWe employed Hierarchical Model Building using Logistic Regression.\n\nModel 1 (Baseline): Tests “Inertia” (does past risk predict future risk?)\nModels 2-4 (Thematic): Test specific domains (Demographics, Built Environment, Spatial Context).\nModel 5 (Integrated): A comprehensive model combining all domains.\n\n\n\nCode\n# --- Model 1: Inertia Only (Lag) ---\nformula_1 &lt;- high_risk ~ lag_mean_monthly\n\n# --- Model 2: Demographics (People) ---\nformula_2 &lt;- high_risk ~ lag_mean_monthly + \n                         pov_rate + renter_share + pct_black + med_rent\n\n# --- Model 3: Built Environment (Place Quality) ---\nformula_3 &lt;- high_risk ~ lag_mean_monthly + \n                         hist_viol_per_1k + hotspot_filings + housing311_per_1k\n\n# --- Model 4: Spatial Context (Location) ---\nformula_4 &lt;- high_risk ~ lag_mean_monthly + \n                         dist_center_km + dist_hotspot_km + neighborhood_cluster\n\n# --- Model 5: Full Integrated Model ---\nformula_5 &lt;- high_risk ~ lag_mean_monthly + \n                         pov_rate + pct_black + renter_share +\n                         hist_viol_per_1k + hotspot_filings + housing311_per_1k +\n                         dist_center_km + neighborhood_cluster\n\n# Train all models on the Training Set\nm1 &lt;- glm(formula_1, data = train_set, family = \"binomial\")\nm2 &lt;- glm(formula_2, data = train_set, family = \"binomial\")\nm3 &lt;- glm(formula_3, data = train_set, family = \"binomial\")\nm4 &lt;- glm(formula_4, data = train_set, family = \"binomial\")\nm5 &lt;- glm(formula_5, data = train_set, family = \"binomial\")\n\n\n\n\n4.2.3 Compare Model Performance\nWe applied three standard statistical metrics to the 2025 Test Set: AUC,Accuracy and AIC.\n\n\nCode\nlibrary(pROC)\nlibrary(caret)\n\n# Function to calculate metrics for a model\ncalc_metrics &lt;- function(model, test_data) {\n  # Predict probabilities\n  probs &lt;- predict(model, newdata = test_data, type = \"response\")\n  # Predict class (0 or 1)\n  preds &lt;- if_else(probs &gt; 0.5, 1, 0)\n  \n  # Calculate Metrics\n  roc_obj  &lt;- roc(test_data$high_risk, probs)\n  auc_val  &lt;- as.numeric(auc(roc_obj))\n  accuracy &lt;- mean(preds == test_data$high_risk)\n  aic_val  &lt;- AIC(model)\n  \n  return(c(AUC = auc_val, Accuracy = accuracy, AIC = aic_val))\n}\n\n# Run comparison\nresults &lt;- data.frame(\n  Model = c(\"M1: Inertia\", \"M2: Demographics\", \"M3: Built Env\", \"M4: Spatial\", \"M5: Full\"),\n  rbind(\n    calc_metrics(m1, test_set),\n    calc_metrics(m2, test_set),\n    calc_metrics(m3, test_set),\n    calc_metrics(m4, test_set),\n    calc_metrics(m5, test_set)\n  )\n)\n\n# Display Table\nlibrary(knitr)\nkable(results, digits = 3, caption = \"Model Comparison Table (Test Set Performance)\")\n\n\n\nModel Comparison Table (Test Set Performance)\n\n\nModel\nAUC\nAccuracy\nAIC\n\n\n\n\nM1: Inertia\n0.971\n0.929\n707.873\n\n\nM2: Demographics\n0.974\n0.912\n661.983\n\n\nM3: Built Env\n0.971\n0.920\n638.294\n\n\nM4: Spatial\n0.975\n0.920\n676.184\n\n\nM5: Full\n0.969\n0.943\n609.176\n\n\n\n\n\nThe results establish Model 5 (Full Integrated) as the champion. While Model 4 (Spatial) achieved a marginally higher AUC (0.975 vs 0.969), Model 5 demonstrated superior Accuracy (94.3%), correctly classifying the vast majority of tracts in the 2025 test set. Crucially, Model 5 achieved the lowest AIC (609.176), significantly outperforming the baseline models (AIC &gt; 700). This drastic drop in AIC confirms that the added complexity—combining demographics, built environment, and spatial features—provides essential explanatory power rather than noise. Therefore, we select Model 5 as the most robust instrument for forecasting 2026 risk.\n\n\n4.2.4 a Visual Comparison (ROC Curves)\n\n\nCode\n# Generate ROC objects\nroc1 &lt;- roc(test_set$high_risk, predict(m1, newdata = test_set, type = \"response\"))\nroc2 &lt;- roc(test_set$high_risk, predict(m2, newdata = test_set, type = \"response\"))\nroc3 &lt;- roc(test_set$high_risk, predict(m3, newdata = test_set, type = \"response\"))\nroc4 &lt;- roc(test_set$high_risk, predict(m4, newdata = test_set, type = \"response\"))\nroc5 &lt;- roc(test_set$high_risk, predict(m5, newdata = test_set, type = \"response\"))\n\n# Plot\nggroc(list(M1=roc1, M2=roc2, M3=roc3, M4=roc4, M5=roc5), size = 1) +\n  labs(title = \"ROC Curve Comparison\",\n       subtitle = \"Which model predicts 2025 risk best?\",\n       color = \"Model Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n4.2.5 Check Multicollinearity\n\n\nCode\nlibrary(car)\n\nvif(m5)\n\n\n                          GVIF Df GVIF^(1/(2*Df))\nlag_mean_monthly      1.036732  1        1.018201\npov_rate              2.428184  1        1.558263\npct_black             4.647796  1        2.155875\nrenter_share          3.104750  1        1.762030\nhist_viol_per_1k      2.282594  1        1.510826\nhotspot_filings       1.306671  1        1.143097\nhousing311_per_1k     2.410423  1        1.552554\ndist_center_km        1.955131  1        1.398260\nneighborhood_cluster 21.083598  4        1.463838\n\n\nVIF diagnostic confirms that Model 5 is statistically free from severe multicollinearity issues.\n\n\n4.2.6 Check Feature Importance\n\n\nCode\nlibrary(caret)\n\n# 1. Calculate Variable Importance\n# For GLM, this calculates the absolute value of the t-statistic (or z-statistic)\nimportance_df &lt;- varImp(m5, scale = FALSE)\n\n# 2. Convert to a clean Data Frame for Plotting\nimportance_plot_df &lt;- importance_df %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Feature\") %&gt;%\n  arrange(desc(Overall)) %&gt;%\n  # Remove the intercept (it's not a feature)\n  filter(Feature != \"(Intercept)\")\n\n# 3. Visualize\nggplot(importance_plot_df, aes(x = reorder(Feature, Overall), y = Overall)) +\n  geom_col(fill = \"#3182bd\") +\n  coord_flip() +  # Horizontal bars are easier to read\n  labs(\n    title = \"Feature Importance: What drives Eviction Risk?\",\n    subtitle = \"Based on Model 5 (Full Integrated Model)\",\n    x = NULL,\n    y = \"Importance Score (Absolute z-statistic)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 11, face = \"bold\"),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nOur feature importance analysis reveals that Historical Inertia and Landlord Behavior are the primary drivers of eviction risk."
  },
  {
    "objectID": "Eviction.html#diagnostics-on-the-winning-model-m5",
    "href": "Eviction.html#diagnostics-on-the-winning-model-m5",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "4.3 Diagnostics on the Winning Model (M5)",
    "text": "4.3 Diagnostics on the Winning Model (M5)\n\n4.3.1 Spatial Autocorrelation Diagnostic\nWe check if the Full Model successfully removed spatial clustering in errors.\n\n\nCode\nlibrary(spdep)\n\n# Extract residuals from the best model\ntrain_set$m5_residuals &lt;- residuals(m5, type = \"deviance\")\n\n# Map residuals to geometry\nresid_sf &lt;- philly_tracts %&gt;%\n  left_join(train_set %&gt;% select(GEOID, m5_residuals), by = \"GEOID\") %&gt;%\n  filter(!is.na(m5_residuals))\n\n# Calculate Moran's I\ncoords &lt;- st_centroid(resid_sf)\nknn_nb &lt;- knearneigh(coords, k = 5)\nlw &lt;- nb2listw(knn2nb(knn_nb), style = \"W\")\n\nmoran_result &lt;- moran.test(resid_sf$m5_residuals, lw)\n\n# Print Result (Ideally p-value &gt; 0.05, or at least Moran's I is close to 0)\nprint(moran_result)\n\n\n\n    Moran I test under randomisation\n\ndata:  resid_sf$m5_residuals  \nweights: lw    \n\nMoran I statistic standard deviate = 6.3875, p-value = 0.00000000008433\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.1150100632     -0.0009478673      0.0003295672 \n\n\nThe Moran’s I test on the residuals yielded a statistic of 0.115 with a p-value &lt; 0.05. The value is relatively close to 0 (which indicates perfect randomness). This suggests that our model has successfully explained the vast majority of spatial patterns in eviction risk.And the result of p-value is still statistically significant.\n\n\n4.3.2 Visualize Spatial Error Patterns (Residual Map)\nWe visualized the Deviance Residuals of Model 5 to identify local pockets of model error. By mapping these residuals, we can diagnose if the model systematically biases its predictions in specific neighborhoods.\n\n\nCode\nggplot(resid_sf) +\n  geom_sf(aes(fill = m5_residuals), color = NA) +\n  scale_fill_gradient2(\n    low = \"#4575b4\",   # Blue: Model predicted High, Actual was Low (Overestimation)\n    mid = \"white\",     # White: Perfect Prediction\n    high = \"#d73027\",  # Red:  Model predicted Low, Actual was High (Underestimation)\n    midpoint = 0,\n    name = \"Deviance\\nResiduals\"\n  ) +\n  labs(\n    title = \"Map of Model Errors (Residuals)\",\n    subtitle = \"Where is Model 5 failing?\",\n    caption = \"Red = Underestimation (Missed Risk) | Blue = Overestimation (False Alarm)\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nWhile the errors show some local clustering (consistent with our Moran’s I result of 0.115), they do not show a city-wide systemic bias. This confirms Model 5 is generally robust.\n\n\n4.3.3 Spatial Cross-Validation (LOGOCV)\nWe used the 5 Neighborhood Clusters (created in Section 4.1.3) as the splitting criteria. In each round of validation, we completely held out one entire cluster of neighborhoods and trained the model on the remaining four clusters. We then asked the model to predict risk in the unseen cluster.\n\n\nCode\nlibrary(caret)\n\n# 1. Define the Control Method: Leave-One-Group-Out\n# We use the 'neighborhood_cluster' we created earlier as the \"Group\"\nfit_control &lt;- trainControl(\n  method = \"CV\", \n  number = 5,              # 5 Clusters\n  savePredictions = TRUE,\n  classProbs = TRUE,       # Needed for ROC/AUC\n  summaryFunction = twoClassSummary, # Optimizes for AUC\n  index = createFolds(train_set$neighborhood_cluster, k = 5) # Critical: Split by Cluster!\n)\n\n# 2. Prepare Data for Caret (Needs \"Yes/No\" factor for classification)\ncv_data &lt;- train_set %&gt;%\n  mutate(high_risk_fac = factor(if_else(high_risk == 1, \"High\", \"Low\"), \n                                levels = c(\"High\", \"Low\"))) %&gt;%\n  drop_na()\n\n# 3. Re-train Model 5 using Spatial CV\n# Note: We use the exact same formula as M5\nmodel_spatial_cv &lt;- train(\n  high_risk_fac ~ lag_mean_monthly + \n                  pov_rate + pct_black + renter_share +\n                  hist_viol_per_1k + hotspot_filings + housing311_per_1k +\n                  dist_center_km, # Removed 'neighborhood_cluster' from predictors because it's the split variable!\n  data = cv_data,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = fit_control,\n  metric = \"ROC\"\n)\n\n# 4. Print Results\nprint(model_spatial_cv)\n\n\nGeneralized Linear Model \n\n1056 samples\n   8 predictor\n   2 classes: 'High', 'Low' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 211, 211, 212, 211, 211 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9105117  0.6142652  0.9394755\n\n\nComparing this to our temporal test result (AUC ~0.969), the drop to 0.91 is expected and healthy. It indicates that while the model relies partly on local history (‘Inertia’), it still retains excellent predictive power (0.91) even when blindfolded to specific neighborhood locations. This confirms the model has learned structural rules about eviction risk."
  },
  {
    "objectID": "Eviction.html#visualize-the-forecast-probability-map",
    "href": "Eviction.html#visualize-the-forecast-probability-map",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "5.1 Visualize the Forecast (Probability Map)",
    "text": "5.1 Visualize the Forecast (Probability Map)\n\n\nCode\nggplot(forecast_map) +\n  geom_sf(aes(fill = pred_prob), color = NA) +\n  scale_fill_viridis_c(\n    option = \"rocket\", \n    direction = -1, \n    name = \"Risk Probability\",\n    labels = percent\n  ) +\n  labs(\n    title = \"Forecast: Eviction Risk Probability (Nov 2025 – Nov 2026)\",\n    subtitle = \"Predicted by Model 5 (Full Integrated Model)\\nDarker/Red areas indicate &gt;80% probability of high eviction activity\",\n    caption = \"Source: Philadelphia Eviction Data\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nHigh-Intensity Clusters: The darkest areas (probabilities &gt; 75%) are tightly clustered in specific corridors of North and West Philadelphia. This indicates that the model is extremely confident that these specific neighborhoods will experience severe eviction pressure in 2026."
  },
  {
    "objectID": "Eviction.html#visualize-the-forecast-binary-risk-map",
    "href": "Eviction.html#visualize-the-forecast-binary-risk-map",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "5.2.1 Visualize the Forecast (Binary Risk Map)",
    "text": "5.2.1 Visualize the Forecast (Binary Risk Map)\nWe use three distinct cutoffs:\n\nAggressive (&gt; 0.3): This casts a wide net to catch as many potential eviction cases as possible, accepting a higher rate of false alarms.\nBalanced (&gt; 0.5): This uses the standard statistical default.\nConservative (&gt; 0.8): This targets only the ‘sure bets’ to minimize wasted resources.\n\n\n\nCode\n# --- 1. Create Multiple Scenarios ---\nfuture_scenarios &lt;- future_input %&gt;%\n  mutate(\n    # Strategy A: High Recall (Catch almost all potential risks, but more false alarms)\n    `Strategy: &gt; 0.3 (Aggressive)` = if_else(pred_prob &gt; 0.3, \"High Risk\", \"Low Risk\"),\n    \n    # Strategy B: Balanced (The statistical default)\n    `Strategy: &gt; 0.5 (Balanced)`   = if_else(pred_prob &gt; 0.5, \"High Risk\", \"Low Risk\"),\n    \n    # Strategy C: High Precision (Target only the absolute worst hotspots)\n    `Strategy: &gt; 0.8 (Conservative)` = if_else(pred_prob &gt; 0.8, \"High Risk\", \"Low Risk\")\n  ) %&gt;%\n  # Convert wide format to long format for plotting\n  pivot_longer(\n    cols = starts_with(\"Strategy\"),\n    names_to = \"Strategy_Name\",\n    values_to = \"Risk_Label\"\n  )\n\n# --- 2. Join to Geometry ---\n# Note: This will triple the number of rows (one geometry per strategy per tract)\nscenario_map &lt;- philly_tracts %&gt;%\n  left_join(future_scenarios, by = \"GEOID\") %&gt;%\n  filter(!is.na(Risk_Label))\n\n# --- 3. Plot Side-by-Side Maps ---\nggplot(scenario_map) +\n  geom_sf(aes(fill = Risk_Label), color = \"white\", lwd = 0.01) +\n  scale_fill_manual(\n    values = c(\"High Risk\" = \"#d73027\", \"Low Risk\" = \"#4575b4\"),\n    name = \"Risk Class\"\n  ) +\n  # This creates the 3 panels\n  facet_wrap(~ Strategy_Name, ncol = 3) + \n  labs(\n    title = \"Policy Trade-offs: Comparing Intervention Thresholds\",\n    subtitle = \"How does the definition of 'High Risk' change our target areas for 2026?\",\n    caption = \"Aggressive (&gt;0.3) = Max Coverage | Balanced (&gt;0.5) = Standard | Conservative (&gt;0.8) = Max Precision\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 11, face = \"bold\") # Make panel titles larger\n  )\n\n\n\n\n\n\n\n\n\nRecommendation: Given the looming 2026 funding cuts, we recommend this ‘High Precision’ strategy for allocating expensive resources (like direct rental assistance), while reserving the ‘Aggressive’ strategy for low-cost interventions (like information campaigns)."
  },
  {
    "objectID": "Eviction.html#conclusion",
    "href": "Eviction.html#conclusion",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "6.1 Conclusion",
    "text": "6.1 Conclusion\n\n\nCode\n# Calculate summary statistics for the report text\nn_high_risk &lt;- sum(future_input$pred_risk_label == \"High Risk\")\npct_high_risk &lt;- round(mean(future_input$pred_risk_label == \"High Risk\") * 100, 1)\n\n# Identify the top 5 highest risk tracts\ntop_risk_tracts &lt;- future_input %&gt;%\n  arrange(desc(pred_prob)) %&gt;%\n  head(5) %&gt;%\n  select(GEOID, pred_prob, lag_mean_monthly, pov_rate)\n\nprint(paste(\"Total High Risk Tracts Predicted:\", n_high_risk))\n\n\n[1] \"Total High Risk Tracts Predicted: 71\"\n\n\nCode\nprint(paste(\"Percentage of City:\", pct_high_risk, \"%\"))\n\n\n[1] \"Percentage of City: 20.2 %\"\n\n\nCode\nprint(top_risk_tracts)\n\n\n# A tibble: 5 × 4\n  GEOID       pred_prob lag_mean_monthly pov_rate\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 42101021800     1.000             12.5   0.148 \n2 42101036100     1.000             11.6   0.0478\n3 42101030100     1.000             13.8   0.442 \n4 42101012100     0.999             10.3   0.169 \n5 42101023900     0.999              9     0.101 \n\n\nOur Model 5 forecast projects that 71 census tracts will be classified as ‘High Risk’ in the upcoming year. These 71 tracts represent exactly 20.2% of the city’s neighborhoods. This finding is remarkably consistent with the Pareto Principle (80/20 rule) we observed in the historical data, confirming that the 2026 crisis will likely remain highly concentrated rather than spreading evenly across the city. For these specific neighborhoods, the model is effectively finding ‘near certainty’ of high eviction activity. These areas should be the immediate first stop for any ‘High Precision’ legal aid deployment, as they represent the absolute epicenter of displacement risk."
  },
  {
    "objectID": "Eviction.html#limitations",
    "href": "Eviction.html#limitations",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "6.2 Limitations",
    "text": "6.2 Limitations\n\nReporting Bias in 311 data: Our model uses 311 complaints as a proxy for housing distress.However, the tenants in the most vulnerable neighborhoods may be less likely to report violations than residents in gentrifying areas.For example, they fear landlord retaliations. So our model may paradoxically interpret a lack of complaints as “stability” in areas that are actually suffering from silent, unreported neglect.\nCensus-Tract Scale, Not Individual Properties: Our predictions operate at the Census Tract level. High-risk classification for a tract does not mean every property in that tract is at risk. So interventions should be verified at the individual property level to avoid wasting resources on stable buildings within distressed areas."
  },
  {
    "objectID": "Eviction.html#recommendations",
    "href": "Eviction.html#recommendations",
    "title": "Eviction Risk Prediction in Philadelphia",
    "section": "6.3 Recommendations",
    "text": "6.3 Recommendations\n\nFor small set of extreme hotspots, we could operate direct rent relief, full legal representation, intensive case management. Also, we need to verify needs at the property/household level with local partners before committing these high-cost resources.\nFor wider ring of at-risk neighborhoods around the hotspots, we can operate low-cost interventions – flyers, text alerts, know-your-rights workshops, landlord engagement. Also we can use this tier to reach areas where 311 distress may be under-reported, and monitor for emerging hotspots to feed back into the model."
  }
]